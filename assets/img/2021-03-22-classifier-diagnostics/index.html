<p><strong>TL;DR:</strong> In applied machine learning, models are often optimised for their accuracy in predicting a target. When the target is quantitative, measures like the R^2 value and the Root Mean Squared Error (RMSE) etc., help in quantifying this accuracy. When the target is qualitative, metrics like Accuracy, Precision, Recall and several others are used. We term these measures ‘classification metrics’ for the sake of this blog post.</p> <p>In this post, I explain classification metrics and provide python code to calculate them. I also produce a report including them that can be easily saved to a file for later use.</p> <p>Say you’ve run your data through a classifier, and the classifier has predicted labels for your data. To judge how accurate those predictions are, you can evaluate them by the metrics explained below</p> <h3 id="section-1-classification-metrics">Section 1: Classification Metrics</h3> <p>The metrics covered by this post is as follows:</p> <ul> <li> <strong>Accuracy</strong> - What proportion of sentences were correctly classified (i.e. as positive, negative, neutral)?</li> <li> <strong>Precision</strong> - Answers the q: What proportion of <em>predicted</em> positives were actual positives? The polar opposite for this is specificity: <ul> <li> <strong>Specificity</strong> - Answers the q: What proportion of <em>predicted</em> negatives were actual negatives?</li> </ul> </li> <li> <strong>Recall/Sensitivity</strong> - Answers the q: What proportion of <em>actual</em> positives were predicted positive?</li> <li> <strong>F-1 Score</strong> - The harmonic mean of a classifier’s precision and recall scores</li> </ul> <p>Generally, when someone mentions the last 3 metrics above, without a prefix, they mean them to be the <em>macro-precision</em>, <em>macro-recall</em>, and <em>macro-F1</em>. These metrics come in two other flavours however, the micro- and the weighted. Let’s take the example of the precision score (but the exact same descriptions can be said for recall and the F-1 score). Whilst the macro precision is simply the average of the precision scores calculated for each class, the weighted precision weights this score by the number of examples of that class before averaging across classes. This is why macro precision is a better metric for balanced data-sets while weighted precision is better for imbalanced ones. Micro precision is more involved than both, and is explained later on.</p> <p>The above metrics can be calculated from a <strong>confusion matrix</strong> which is a 2D matrix showing the predicted and actual counts of a target category, therefore showing the distribution of True Positives (TPs), False Positives (FPs), True Negatives (TNs), and False Negatives (FNs) in your data.</p> <p>As we see in Section 1.1 below, the equations of the above metrics require knowledge of the TP/FPs and TN/FNs. Calculating the above metrics by hand therefore requires a confusion matrix to be made beforehand.</p> <h4 id="section-11-diving-deeper-into-the-metrics">Section 1.1: Diving Deeper into the Metrics</h4> <p>Let’s display a confusion matrix for a binary classification (i.e. 2 classes for the target) problem to intuitively get a grasp of its contents.</p> <p><img src="CM1.png" alt="Confusion Matrix"></p> <p>The goal for a classifier is to increase TP/TNs while reducing misclassification, or the number of FP/FNs</p> <p>Knowing the above, our metrics can be represented as the following equations: \(Accuracy = \frac{TP + TN}{FP + FN + TP + TN}\)</p> \[Precision = \frac{TP}{TP + FP}\] \[Recall = \frac{TP}{TP + FN}\] \[F_1= \frac{2}{\frac{1}{Precision} \times \frac{1}{Recall}} = 2 \times \frac{Recall \times Precision}{Recall + Precision}\] <h4 id="section-12-diving-deeper-into-various-flavours-of-precision">Section 1.2: Diving Deeper into various flavours of Precision</h4> <p>Shown above is the equation for the precision score of <strong>one class</strong>.</p> <p>The macro and weighted precision scores for a binary classification (i.e. with 2 classes) can be represented as:</p> \[Macro Precision= \frac{Precision_{class1} + Precision_{class2}}{2}\] \[Weighted Precision= \frac{(Precision_{class1} \times c_1) + (Precision_{class2} \times c_2)}{c_1 + c_2}\] <p>where $c_1$ and $c_2$ are the number of examples in classes $1$ and $2$</p> <p>Note that if we replace <code class="language-plaintext highlighter-rouge">Precision</code> above with <code class="language-plaintext highlighter-rouge">F-1 score</code> or <code class="language-plaintext highlighter-rouge">Recall</code>, the formulas would still work!</p> <p>To calculate micro precision, instead of taking measurements of precision <strong>for each class</strong> (i.e. taking TPs and FPs for each class), we compute TPs and FPs out of the <strong>total pool</strong> of examples.</p> <p>Take the confusion matrix below for another binary classification problem, classifying photos of a cat or a dog. <img src="confusion_matrix.png" alt="confusion matrix 2"></p> <p>We calculate the TP and FP out of the total number of examples as follows: <img src="TP+FP.png" alt="TP+FP"></p> <p>And therefore our micro precision is: <img src="microprecision.png" alt="microprecision"></p> <h4 id="section-13-the-rocauc-scores">Section 1.3: The ROC/AUC scores</h4> <p>In a later blog post, we’ll explore two additional metrics briefly described below, as they are derived simultaneously while training a machine learning model.</p> <ul> <li> <strong>ROC curve</strong> - Shows how changing the <a href="https://developers.google.com/machine-learning/crash-course/classification/thresholding" target="_blank" rel="noopener noreferrer">classification threshold</a> changes the True Positive Rate and False Positive Rate. In a binary classification problem, the classification threshold is a probability value that delineates between two classes, and is a hyperparameter we have to optimise for. Why is it a probability value? Well that’s because classifiers return probability values before we associate a category to those values based on our interpretation of them! Say, for one image classified by our Dog vs. Cat image classifier above, the resulting probabilities are 0.95 that it’s a dog and 0.05 that it’s a cat. The image is <strong>likely</strong> a dog…but what if we had a second image where the probability of it being a dog was 0.6? The <strong>classification threshold</strong> we set determines what class this second image belongs to, and if we set it to be 0.6, then our image <em>just barely made it to the threshold of being a dog image</em>. Probabilities below 0.6 will classify it as a cat. You can see how changing this threshold might change the number of TPs/FPs and TNs/FNs in our data. See the image below for a visual depiction of this curve.</li> <li> <strong>AUC</strong> - The area under the ROC curve. A high AUC value, associated with a ROC curve occupying the upper left quadrant is ideal and indicates a good classifier (i.e. because its True Positive Rate is high, whilst its False Positive Rate is low)</li> </ul> <p><img src="roc-curve-v2.png" alt="ROC-AUC curves"></p> <h4 id="what-metric-do-we-choose-to-evaluate-our-classifier-by">What metric do we choose to evaluate our classifier by?</h4> <p>It ultimately depends on your research question. If you’re classifying who has diabetes from a patient pool, for example, you’d want recall to be high. If you’re recommending movies to a user, you’d want the user to like the movies you recommended, so you’d want precision to be high. Overall, with a balanced data-set, you’d want accuracy to be high. With an un-balanced data-set, you’d want the weighted F1-score to be high.</p> <h3 id="section-2-code">Section 2: Code</h3> <p>In this section, we write some code to generate the metrics above.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#first we import necessary packages
</span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>After training a classifier to generate predictions, we compare predictions to true labels and apply the metrics above.</p> <p>Below, we read in a file containing predictions and true labels for a sentiment classification problem (i.e. classifying the sentiment of text entries as either positive, negative or neutral). This is a multi-class problem, as we have more than 2 classes to predict.</p> <p>In the resulting data-frame, we have a column housing true labels (i.e. <code class="language-plaintext highlighter-rouge">sentiment</code>), and one housing predictions (i.e. <code class="language-plaintext highlighter-rouge">predictions</code>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#insert the data-set you'd want to read
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'predictions.csv'</span><span class="p">)</span>
</code></pre></div></div> <p>We convert numerical labels generated by our classifier to categorical labels for ease of interpretation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sentiment_to_categories_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s">'positive'</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s">'neutral'</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s">'negative'</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">sentiment</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sentiment</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">sentiment</span><span class="p">:</span> <span class="n">sentiment_to_categories_dict</span><span class="p">[</span><span class="n">sentiment</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Unnamed: 0</th> <th>entry</th> <th>sentiment</th> <th>predictions</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0</td> <td>Up early ferry to Rangitoto walked up snack at...</td> <td>positive</td> <td>neutral</td> </tr> <tr> <th>1</th> <td>1</td> <td>P home on time. Nap then food shop girls over ...</td> <td>neutral</td> <td>negative</td> </tr> <tr> <th>2</th> <td>2</td> <td>Morning nap did not get up until late shower t...</td> <td>neutral</td> <td>neutral</td> </tr> <tr> <th>3</th> <td>3</td> <td>N. Slept well but tired. Busy board at start b...</td> <td>neutral</td> <td>neutral</td> </tr> <tr> <th>4</th> <td>4</td> <td>N. Slept well woken by post. Slow but steady n...</td> <td>neutral</td> <td>neutral</td> </tr> </tbody> </table> </div> <p>We store the predictions and true labels on separate vectors</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#True values (actual)
</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'sentiment'</span><span class="p">]</span>

<span class="c1">#predicted values (from model output)
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">]</span>
</code></pre></div></div> <p>We then create row and column labels to append to our confusion matrix later on</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#get row and column labels for confusion matrix
</span>
<span class="c1">#get unique row labels
</span><span class="n">row_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

<span class="c1">#get column labels
</span><span class="n">column_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label</span> <span class="o">+</span> <span class="s">"_predicted"</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">row_labels</span><span class="p">]</span>
</code></pre></div></div> <p>Here we create a confusion matrix, passing in the row and column labels we create above</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#create a confusion matrix object and display it (with labels)
</span><span class="n">c_m</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">row_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">column_labels</span><span class="p">)</span>
</code></pre></div></div> <p>We display the confusion matrix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c_m</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>negative_predicted</th> <th>neutral_predicted</th> <th>positive_predicted</th> </tr> </thead> <tbody> <tr> <th>negative</th> <td>6</td> <td>26</td> <td>3</td> </tr> <tr> <th>neutral</th> <td>12</td> <td>40</td> <td>10</td> </tr> <tr> <th>positive</th> <td>8</td> <td>21</td> <td>2</td> </tr> </tbody> </table> </div> <p>…as a data-frame…with all of the other metrics mentioned above:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#display all diagnostics as a data-frame, save results to a variable too
</span><span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">confusion_matrix</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>negative</th> <th>neutral</th> <th>positive</th> <th>accuracy</th> <th>macro avg</th> <th>weighted avg</th> </tr> </thead> <tbody> <tr> <th>precision</th> <td>0.230769</td> <td>0.459770</td> <td>0.133333</td> <td>0.375</td> <td>0.274624</td> <td>0.318094</td> </tr> <tr> <th>recall</th> <td>0.171429</td> <td>0.645161</td> <td>0.064516</td> <td>0.375</td> <td>0.293702</td> <td>0.375000</td> </tr> <tr> <th>f1-score</th> <td>0.196721</td> <td>0.536913</td> <td>0.086957</td> <td>0.375</td> <td>0.273530</td> <td>0.334918</td> </tr> <tr> <th>support</th> <td>35.000000</td> <td>62.000000</td> <td>31.000000</td> <td>0.375</td> <td>128.000000</td> <td>128.000000</td> </tr> </tbody> </table> </div> <p><strong>Notice</strong> that another metric called <code class="language-plaintext highlighter-rouge">support</code> is included in the classification report. Support is simply the number of examples for each class.</p> <p>We can save this full classification report to a file, for later indexing/reporting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#save results to a file
</span><span class="n">confusion_matrix</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'fileName.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>