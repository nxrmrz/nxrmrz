<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://nxrmrz.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nxrmrz.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-11-20T02:14:21+00:00</updated><id>https://nxrmrz.github.io/feed.xml</id><title type="html">Nicole Ramirez</title><subtitle>My digital garden </subtitle><entry><title type="html">Automating myself out of a role</title><link href="https://nxrmrz.github.io/blog/2022/report-automation/" rel="alternate" type="text/html" title="Automating myself out of a role"/><published>2022-11-01T10:05:00+00:00</published><updated>2022-11-01T10:05:00+00:00</updated><id>https://nxrmrz.github.io/blog/2022/report-automation</id><content type="html" xml:base="https://nxrmrz.github.io/blog/2022/report-automation/"><![CDATA[<p>This is what Iâ€™ve designed and built recently to automate myself out of a process that we used to do very manually at <a href="https://chnnl.app/home">chnnl</a>:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-11-01/chnnlreportarchi-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-11-01/chnnlreportarchi-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-11-01/chnnlreportarchi-1400.webp"/> <img src="/assets/img/2021-11-01/chnnlreportarchi.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1. chnnl's automated reporting architecture I've built </div> <p>Thereâ€™s a lot going on above, but what you see is a reporting workflow we have tried to automate for 2.5 years. The motivation to automate can be summarised by the ff:</p> <ol> <li>the report is a key service offering of chnnl</li> <li>the data and insights chnnl wanted out of it increasingly matured and stabilised over time</li> <li>the report took analysts anywhere from 30 minutes to 2 hours to produce and validate which wasnâ€™t a scaleable duration</li> <li>aspects of the report generation process were repetitive and deterministic and could be codified</li> </ol> <p>At the tail end of this automation, I can proudly report some technical and business learnings from having embarked and led this full-stack project. While iâ€™m planning to write separate posts breaking down all of my learnings, for now, Iâ€™ll briefly talk about the architecture Iâ€™ve adopted for the project, what it does, and why I made the choice to go with certain components I went with.</p> <p>Read on if you are wanting a very high level overview of what cloud microservices go into a workflow like this, or are interested in general cloud architecture work like me.</p> <p>This post is inspired by AWSâ€™ <a href="https://aws.amazon.com/architecture/this-is-my-architecture/?tma.sort-by=item.additionalFields.airDate&amp;tma.sort-order=desc&amp;awsf.category=*all&amp;awsf.industry=*all&amp;awsf.language=*all&amp;awsf.show=*all">This Is My Architecture</a> series, where architects or leaders from different companies speak about architectural decisions theyâ€™ve made on AWS that power their core products and business. I highly recommend browsing through the series when you research suitable architectures for your own AWS project.</p> <h1 id="whats-this-architecture-for">Whatâ€™s this architecture for?</h1> <p>The architecture above is for an automated reporting system that outputs summarised wellbeing data for chnnl clients in a multi-page excel sheet. The main users of the system are chnnlâ€™s customer care team, who produce reports from this and use them downstream for conversations with clients on workplace wellbeing. A customer care manager at chnnl, the user of the reporting system, would produce a report per client per recurring period of time (i.e. a report for client A every 2 weeks, client B every month, etc), and therefore the reporting date range and client name are parameters to the system. Given these parameters, the system fetches data from chnnlâ€™s secured data lake, applies transformations and aggregations on the data, writes the results to an excel file, and sends the result back to the user in less than a minute. The user interacts with a web form and sees the result displayed on this form.</p> <h1 id="pre-automation">Pre-Automation</h1> <p>Before this automation, an analyst would take various manual steps to produce the report. The analyst would query raw report data with SQL, download the data to their local machine, fire up an interactive R session and run R scripts that transform and aggregate the data, then copy the transformed results onto an excel fileâ€“very <strong>manual, unscaleable</strong> and <strong>error-prone</strong>. This was done in the early days of the report as the information contained in the report changed a lot as per user requirements, so the analyst had to write functions to accommodate. There were few clients opting into the report service then as well, so doing it all manually wasnâ€™t a major timesink yet.</p> <h1 id="need-to-automate">Need to automate</h1> <p>As chnnlâ€™s clientele grew and the requirements reached a stable point, automation increasingly made sense. We started to think about architectures to automate this system and the only technical considerations were:</p> <ul> <li>the only manual point should be the entering of the report parameters. Everything else has to be automated.</li> <li>we had to keep the data transformations in R, the language they were originally written in for stability. Rewriting to python for example would take time</li> <li>every component is provisioned with Infrastructure as Code (IaC)</li> </ul> <h1 id="architecture">Architecture</h1> <p>First, Iâ€™ll walk through what the flow involves.</p> <ol> <li>A user interacts with a web form, inputting parameters that are needed to produce a report</li> <li>When the user submits the form, these parameters are posted to an API route <code class="language-plaintext highlighter-rouge">POST /report</code>.</li> <li>This route triggers an AWS Step Function workflow, which causes the report to be produced. This flow chains various AWS lambda functions doing various tasks: 1.) querying AWS Athena for raw data 2.) performing ETL on this data and producing the final excel report and 3.) outputting an authenticated S3 URL to download this report.</li> <li>There is another API route that gets invoked every few seconds by the form (<code class="language-plaintext highlighter-rouge">GET /result</code>) which polls and returns the status of the reporting workflow. Once the workflow completes, the authenticated URL is made available to the form. Otherwise, a pending status is displayed, as are errors, which are additionally broadcasted on a slack channel monitored by chnnl devs for easier visibility and debugging.</li> </ol> <h1 id="choosing-the-architecture">Choosing the architecture</h1> <p>Now Iâ€™ll talk through the different components I used to build the flow and briefly explain why I chose them.</p> <h4 id="lambda">Lambda</h4> <p>Lambda functions allow you to run code without worrying about provisioning servers. Any code involving business logic were transferred to lambdas in this project. In the report generation flow, I had three lambdas: 1.) a lambda querying input data from chnnlâ€™s data lake with SQL 2.) a lambda transforming and aggregating data and creating the report 3.) a lambda creating an authenticated, pre-signed URL of the report for download later. I also had lambdas for authenticating HTTP requests to my report API, as well as polling the status of the reporting workflow. Lambdas made sense for running functions as they are severless and low-cost. They supported the creation of custom language runtimes like the one I made to accommodate my ETL scripts in R, and for python runtimes, had <code class="language-plaintext highlighter-rouge">boto3</code>, the SDK for AWS natively installed. An alternative to lambda obviously is running an always-on server that hosted my reporting code, which was unnecessary because report production is on-demand.</p> <h4 id="api-gateway">API Gateway</h4> <p>As opposed to running the report on schedule, an API was needed to produce the report on-demand. The parameters to the report had to be passed to the API. I created two routes on a single HTTP API in API Gateway, one for starting report production (<code class="language-plaintext highlighter-rouge">POST /report</code>), and the other for polling its status (<code class="language-plaintext highlighter-rouge">GET /result</code>). This made sense vs creating two separate APIs, as both related to the same task â€“ the reporting task. I used an HTTP as opposed to REST or websocket API as I wanted something low cost, that I didnt need to configure as much, and my use case didnt involve duplex communication. I also had to create a third route on the same API to handle CORS requests from browsers, which were sent due to the presence of Bearer tokens on my API request headers for added security. All routes had an authorization lambda that authorised the Bearer token and an AWS service integration that carries out the backend task attached to the API route.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-11-01/reportingapi-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-11-01/reportingapi-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-11-01/reportingapi-1400.webp"/> <img src="/assets/img/2021-11-01/reportingapi.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2. Three separate routes on one API. Each are authorised by a lambda, and have a unique backend integration attached </div> <h4 id="step-function-workflow">Step Function Workflow</h4> <p>Step Functions (SF) were used in this context as a workflow orchestration tool, to chain together the three lambdas producing the report. The alternative to SF is to have one lambda run the two other lambdas, but this makes code non-modular. SF allows us to pass data from one lambda to another and debug precisely where in the reporting workflow did errors originate. An open source equivalent of SF would be airflow or prefect, for example, which shows processes in a DAG like SF does. Our usage of SF here is very simpleâ€“just chaining lambdas togetherâ€“however, SF allows for conditional workflows, retrying processes when they fail, and many other processes. SF also conveniently integrates directly with API gateway, so that the <code class="language-plaintext highlighter-rouge">POST /report</code> endpoint passes data directly to the SF workflow, as well as AWS EventBridge, which can listen to any errors SF generates.</p> <div style="text-align: center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-11-01/sf-flow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-11-01/sf-flow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-11-01/sf-flow-1400.webp"/> <img src="/assets/img/2021-11-01/sf-flow.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3. We can debug, through the DAG of the worklow, precisely where an error happened (i.e. in the first step, QueryJobDev here) </div> <h4 id="others">Others</h4> <ul> <li><strong>S3</strong>: used as the storage mechanism for dumping intermediate and final report data. The final report was downloadable via a pre-signed S3 URL of the final report file. Iâ€™ve used one bucket to hold everything related to the report, with different folder structures to house different artefacts. Iâ€™ve also enabled bucket tiering to save costs, such that objects in the bucket that are unused for &gt;3 months are transferred to a cheaper bucket used for archiving.</li> <li><strong>AWS ECR</strong>: used to host the docker container for running the ETL scripts written in R. The R lambda Iâ€™ve created runs this docker container every time itâ€™s invoked. Iâ€™ll detail the R lambda creation in a separate post, but Iâ€™ve used an Amazon Managed Image (Linux, al2) as my base to build the container.</li> <li><strong>Cloudwatch Logs</strong>: used extensively to debug during the development process. Iâ€™ve manually enabled my APIs to stream to logs while developing so I can debug what aspects of the API errors out (i.e. is it the step function workflow, the API authorization process, a client-side error, etc?). This will turn off in production to save costs. Iâ€™ve kept logs on in services that have them turned on by default (i.e. Lambda).</li> <li><strong>AWS EventBridge</strong>: used to create an event rule, a pattern that detects when the step function workflow errors out/gets cancelled. If this pattern is met, event bridge broadcasts a message to an event listener. I used this workflow to broadcast production errors of the reporting system to a slack channel, as detailed below.</li> <li><strong>Cloudformation</strong>: used to build and deploy the entire architecture with code (infrastructure as code). The architecture was created as a cloudformation stack, which can be synthesised, deployed and destroyed with simple CLI commands. Instead of using cloudformation templates, I used AWS CDK as this was a pre-existing devops choice chnnl adopted, which Iâ€™m happy with, as CDK is available in typescript and python. This made provisioning infrastructure easier as I didnâ€™t have to learn another language like i would using cloudformation/ARM templates for deployment for example.</li> <li><strong>Athena</strong>: I didnâ€™t create this, but this was available as a way to interactively access our data-lake via SQL commands. I built the reporting system to run SQL code to query Athena automatically using saved Athena queries. One thing to note is that Athena is built as a UI/IDE to run SQL commands interactively for analysts, and therefore might be costly to use at scale. When this happens, one would likely forego Athena and run queries against a data warehouse for eg.</li> </ul> <h4 id="monitoring-with-sns-event-bridge-cloudtrail">Monitoring with SNS, Event Bridge, Cloudtrail</h4> <p>When running microservices, monitoring crucial potential failure points is necessary. As mentioned before, some services like lambda have log streaming available by default. Not all of those logs will be useful though, and those logs are in the AWS server and not in a place we can easily access or view. For this reporting system, Iâ€™ve tried to account for errors client-side by form validation mechanisms in the web form Iâ€™ve created. Because of the tight timeline of the project, I reasoned that I canâ€™t enumerate all possible errors that may escape that client-side validation, or may originate server side, so itâ€™s better for me to have a catch-all mechanism that broadcasts errors to a place I can see/access. I could then develop better error handling mechanisms after seeing what errors come up time and again.</p> <p>This catch-all mechanism was developed with a combination of <strong>AWS Event Bridge (EB)</strong> and <strong>AWS Simple Notification Service (SNS)</strong>. I created various rules in EB that detect a <code class="language-plaintext highlighter-rouge">FAIL</code>, <code class="language-plaintext highlighter-rouge">CANCEL</code>, <code class="language-plaintext highlighter-rouge">ABORT</code> status for any step of my step function reporting workflow, and a rule detecting the absence of a report file in S3 after the workflow is run, which can indicate failure for any number of reasons not captured by the previous rule. These event failure rules in EB were tied to event listeners in SNS, which, in response to a broadcasted event from EB, send the details of the event to a <code class="language-plaintext highlighter-rouge">development</code> and <code class="language-plaintext highlighter-rouge">production</code> slack thread in chnnl. This allows monitoring of failures in dev and prod as they happen, which is handy for debugging on the spot.</p> <p>Finally, I used <strong>AWS Cloudtrail</strong> extensively during development to translate how the creation of different microservices in the console can be reproduced and codified in AWS CDK. When developing any AWS service, I often quickly PoC the service on the console to give me an idea of how to create and test it, then codify the service with AWS CDK later when it is tested and working.</p> <p>An example is the creation of an API integration on the console and CDK. To preserve all of the configurations that are baked in by default when creating that service in the console, I often inspect Cloudtrail logs generated during the console creation event. I then make sure that all the necessary parameters are copied over to CDK, because otherwise the integration might not work as well as it does on the console (Fig 3 below). Cloudtrail contains information about <em>all events executed in an AWS plan</em>.</p> <div class="row mt-2"> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-11-01/cloudtrailparams-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-11-01/cloudtrailparams-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-11-01/cloudtrailparams-1400.webp"/> <img src="/assets/img/2021-11-01/cloudtrailparams.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-2 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-11-01/cdkintegration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-11-01/cdkintegration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-11-01/cdkintegration-1400.webp"/> <img src="/assets/img/2021-11-01/cdkintegration.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Making sure integration parameters like integration_type, payload_format, connection_type, etc are the same between a console created integration (left) and a CDK created one (right) </div> <h1 id="conclusion">Conclusion</h1> <p>This post summarises architectural components and choices I made to design a reporting system that was previously very manually done. Iâ€™d love to extend this post by showing you costing, or how much it takes to run one workflow end-to-end in AWS. Iâ€™d also love to share some research I did on open source equivalents of the above workflow, as well as thoughts on how to scale it up. Those will be in a future post so stay tuned!</p> <p>I actually havenâ€™t automated myself fully from chnnl, just from the reporting I used to do very manually. Hours of report production are now delegated to a system in the cloud, freeing me up to focus on other full-stack engineering and machine learning projects that may provide value to the business. As an engineer Iâ€™ll definitely view every project now as amenable to automation, and carefully design my architecture and craft my code to make it flexible for this in the future.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I've automated a previously manual reporting workflow on AWS. Here's the architecture of the system I've automated.]]></summary></entry><entry><title type="html">Classifier Diagnostics</title><link href="https://nxrmrz.github.io/blog/2021/classifier-diagnostics/" rel="alternate" type="text/html" title="Classifier Diagnostics"/><published>2021-03-22T10:00:16+00:00</published><updated>2021-03-22T10:00:16+00:00</updated><id>https://nxrmrz.github.io/blog/2021/classifier-diagnostics</id><content type="html" xml:base="https://nxrmrz.github.io/blog/2021/classifier-diagnostics/"><![CDATA[<p><strong>TL;DR:</strong> In applied machine learning, models are often optimised for their accuracy in predicting a target. When the target is quantitative, measures like the \(R^2\) value and the Root Mean Squared Error (RMSE) etc., help in quantifying this accuracy. When the target is qualitative, metrics like Accuracy, Precision, Recall and several others are used. We term these measures <em>classification metrics</em> for the sake of this blog post.</p> <p>In this post, I explain classification metrics and provide python code to calculate them. I also produce a report including them that can be easily saved to a file for later use.</p> <p>Say youâ€™ve run your data through a classifier, and the classifier has predicted labels for your data. To judge how accurate those predictions are, you can evaluate them by the metrics explained below</p> <h3 id="section-1-classification-metrics">Section 1: Classification Metrics</h3> <p>The metrics covered by this post is as follows:</p> <ul> <li><strong>Accuracy</strong> - What proportion of sentences were correctly classified (i.e. as positive, negative, neutral)?</li> <li><strong>Precision</strong> - Answers the q: What proportion of <em>predicted</em> positives were actual positives? The polar opposite for this is specificity: <ul> <li><strong>Specificity</strong> - Answers the q: What proportion of <em>predicted</em> negatives were actual negatives?</li> </ul> </li> <li><strong>Recall/Sensitivity</strong> - Answers the q: What proportion of <em>actual</em> positives were predicted positive?</li> <li><strong>F-1 Score</strong> - The harmonic mean of a classifierâ€™s precision and recall scores</li> </ul> <p>Generally, when someone mentions the last 3 metrics above, without a prefix, they mean them to be the <em>macro-precision</em>, <em>macro-recall</em>, and <em>macro-F1</em>. These metrics come in two other flavours however, the micro- and the weighted. Letâ€™s take the example of the precision score (but the exact same descriptions can be said for recall and the F-1 score). Whilst the macro precision is simply the average of the precision scores calculated for each class, the weighted precision weights this score by the number of examples of that class before averaging across classes. This is why macro precision is a better metric for balanced data-sets while weighted precision is better for imbalanced ones. Micro precision is more involved than both, and is explained later on.</p> <p>The above metrics can be calculated from a <strong>confusion matrix</strong> which is a 2D matrix showing the predicted and actual counts of a target category, therefore showing the distribution of True Positives (TPs), False Positives (FPs), True Negatives (TNs), and False Negatives (FNs) in your data.</p> <p>As we see in Section 1.1 below, the equations of the above metrics require knowledge of the TP/FPs and TN/FNs. Calculating the above metrics by hand therefore requires a confusion matrix to be made beforehand.</p> <h4 id="section-11-diving-deeper-into-the-metrics">Section 1.1: Diving Deeper into the Metrics</h4> <p>Letâ€™s display a confusion matrix for a binary classification (i.e. 2 classes for the target) problem to intuitively get a grasp of its contents.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/CM1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/CM1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/CM1-1400.webp"/> <img src="/assets/img/2021-03-22-classifier-diagnostics/CM1.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The goal for a classifier is to increase TP/TNs while reducing misclassification, or the number of FP/FNs</p> <p>Knowing the above, our metrics can be represented as the following equations:</p> \[Accuracy = \frac{TP + TN}{FP + FN + TP + TN}\] \[Precision = \frac{TP}{TP + FP}\] \[Recall = \frac{TP}{TP + FN}\] \[F_1= \frac{2}{\frac{1}{Precision} \times \frac{1}{Recall}} = 2 \times \frac{Recall \times Precision}{Recall + Precision}\] <h4 id="section-12-diving-deeper-into-various-flavours-of-precision">Section 1.2: Diving Deeper into various flavours of Precision</h4> <p>Shown above is the equation for the precision score of <strong>one class</strong>.</p> <p>The macro and weighted precision scores for a binary classification (i.e. with 2 classes) can be represented as:</p> \[Macro Precision= \frac{Precision_{class1} + Precision_{class2}}{2}\] \[Weighted Precision= \frac{(Precision_{class1} \times c_1) + (Precision_{class2} \times c_2)}{c_1 + c_2}\] <p>where \(c_1\) and \(c_2\) are the number of examples in classes \(1\) and \(2\)</p> <p>Note that if we replace <code class="language-plaintext highlighter-rouge">Precision</code> above with <code class="language-plaintext highlighter-rouge">F-1 score</code> or <code class="language-plaintext highlighter-rouge">Recall</code>, the formulas would still work!</p> <p>To calculate micro precision, instead of taking measurements of precision <strong>for each class</strong> (i.e. taking TPs and FPs for each class), we compute TPs and FPs out of the <strong>total pool</strong> of examples.</p> <p>Take the confusion matrix below for another binary classification problem, classifying photos of a cat or a dog.</p> <p>We calculate the TP and FP out of the total number of examples as follows:</p> <p>And therefore our micro precision is:</p> <h4 id="section-13-the-rocauc-scores">Section 1.3: The ROC/AUC scores</h4> <p>In a later blog post, weâ€™ll explore two additional metrics briefly described below, as they are derived simultaneously while training a machine learning model.</p> <ul> <li><strong>ROC curve</strong> - Shows how changing the <a href="https://developers.google.com/machine-learning/crash-course/classification/thresholding">classification threshold</a> changes the True Positive Rate and False Positive Rate. In a binary classification problem, the classification threshold is a probability value that delineates between two classes, and is a hyperparameter we have to optimise for. Why is it a probability value? Well thatâ€™s because classifiers return probability values before we associate a category to those values based on our interpretation of them! Say, for one image classified by our Dog vs. Cat image classifier above, the resulting probabilities are 0.95 that itâ€™s a dog and 0.05 that itâ€™s a cat. The image is <strong>likely</strong> a dogâ€¦but what if we had a second image where the probability of it being a dog was 0.6? The <strong>classification threshold</strong> we set determines what class this second image belongs to, and if we set it to be 0.6, then our image <em>just barely made it to the threshold of being a dog image</em>. Probabilities below 0.6 will classify it as a cat. You can see how changing this threshold might change the number of TPs/FPs and TNs/FNs in our data. See the image below for a visual depiction of this curve.</li> <li><strong>AUC</strong> - The area under the ROC curve. A high AUC value, associated with a ROC curve occupying the upper left quadrant is ideal and indicates a good classifier (i.e. because its True Positive Rate is high, whilst its False Positive Rate is low)</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/roc-curve-v2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/roc-curve-v2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2021-03-22-classifier-diagnostics/roc-curve-v2-1400.webp"/> <img src="/assets/img/2021-03-22-classifier-diagnostics/roc-curve-v2.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="what-metric-do-we-choose-to-evaluate-our-classifier-by">What metric do we choose to evaluate our classifier by?</h4> <p>It ultimately depends on your research question. If youâ€™re classifying who has diabetes from a patient pool, for example, youâ€™d want recall to be high. If youâ€™re recommending movies to a user, youâ€™d want the user to like the movies you recommended, so youâ€™d want precision to be high. Overall, with a balanced data-set, youâ€™d want accuracy to be high. With an un-balanced data-set, youâ€™d want the weighted F1-score to be high.</p> <h3 id="section-2-code">Section 2: Code</h3> <p>In this section, we write some code to generate the metrics above.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#first we import necessary packages
</span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>After training a classifier to generate predictions, we compare predictions to true labels and apply the metrics above.</p> <p>Below, we read in a file containing predictions and true labels for a sentiment classification problem (i.e. classifying the sentiment of text entries as either positive, negative or neutral). This is a multi-class problem, as we have more than 2 classes to predict.</p> <p>In the resulting data-frame, we have a column housing true labels (i.e. <code class="language-plaintext highlighter-rouge">sentiment</code>), and one housing predictions (i.e. <code class="language-plaintext highlighter-rouge">predictions</code>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#insert the data-set you'd want to read
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'predictions.csv'</span><span class="p">)</span>
</code></pre></div></div> <p>We convert numerical labels generated by our classifier to categorical labels for ease of interpretation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sentiment_to_categories_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s">'positive'</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s">'neutral'</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s">'negative'</span>
<span class="p">}</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">sentiment</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">sentiment</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">sentiment</span><span class="p">:</span> <span class="n">sentiment_to_categories_dict</span><span class="p">[</span><span class="n">sentiment</span><span class="p">])</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>Unnamed: 0</th> <th>entry</th> <th>sentiment</th> <th>predictions</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>0</td> <td>Up early ferry to Rangitoto walked up snack at...</td> <td>positive</td> <td>neutral</td> </tr> <tr> <th>1</th> <td>1</td> <td>P home on time. Nap then food shop girls over ...</td> <td>neutral</td> <td>negative</td> </tr> <tr> <th>2</th> <td>2</td> <td>Morning nap did not get up until late shower t...</td> <td>neutral</td> <td>neutral</td> </tr> <tr> <th>3</th> <td>3</td> <td>N. Slept well but tired. Busy board at start b...</td> <td>neutral</td> <td>neutral</td> </tr> <tr> <th>4</th> <td>4</td> <td>N. Slept well woken by post. Slow but steady n...</td> <td>neutral</td> <td>neutral</td> </tr> </tbody> </table> </div> <p>We store the predictions and true labels on separate vectors</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#True values (actual)
</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'sentiment'</span><span class="p">]</span>

<span class="c1">#predicted values (from model output)
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'predictions'</span><span class="p">]</span>
</code></pre></div></div> <p>We then create row and column labels to append to our confusion matrix later on</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#get row and column labels for confusion matrix
</span>
<span class="c1">#get unique row labels
</span><span class="n">row_labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

<span class="c1">#get column labels
</span><span class="n">column_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">label</span> <span class="o">+</span> <span class="s">"_predicted"</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">row_labels</span><span class="p">]</span>
</code></pre></div></div> <p>Here we create a confusion matrix, passing in the row and column labels we create above</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#create a confusion matrix object and display it (with labels)
</span><span class="n">c_m</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="n">row_labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">column_labels</span><span class="p">)</span>
</code></pre></div></div> <p>We display the confusion matrix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c_m</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>negative_predicted</th> <th>neutral_predicted</th> <th>positive_predicted</th> </tr> </thead> <tbody> <tr> <th>negative</th> <td>6</td> <td>26</td> <td>3</td> </tr> <tr> <th>neutral</th> <td>12</td> <td>40</td> <td>10</td> </tr> <tr> <th>positive</th> <td>8</td> <td>21</td> <td>2</td> </tr> </tbody> </table> </div> <p>â€¦as a data-frameâ€¦with all of the other metrics mentioned above:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#display all diagnostics as a data-frame, save results to a variable too
</span><span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">confusion_matrix</span>
</code></pre></div></div> <div> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>negative</th> <th>neutral</th> <th>positive</th> <th>accuracy</th> <th>macro avg</th> <th>weighted avg</th> </tr> </thead> <tbody> <tr> <th>precision</th> <td>0.230769</td> <td>0.459770</td> <td>0.133333</td> <td>0.375</td> <td>0.274624</td> <td>0.318094</td> </tr> <tr> <th>recall</th> <td>0.171429</td> <td>0.645161</td> <td>0.064516</td> <td>0.375</td> <td>0.293702</td> <td>0.375000</td> </tr> <tr> <th>f1-score</th> <td>0.196721</td> <td>0.536913</td> <td>0.086957</td> <td>0.375</td> <td>0.273530</td> <td>0.334918</td> </tr> <tr> <th>support</th> <td>35.000000</td> <td>62.000000</td> <td>31.000000</td> <td>0.375</td> <td>128.000000</td> <td>128.000000</td> </tr> </tbody> </table> </div> <p><strong>Notice</strong> that another metric called <code class="language-plaintext highlighter-rouge">support</code> is included in the classification report. Support is simply the number of examples for each class.</p> <p>We can save this full classification report to a file, for later indexing/reporting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#save results to a file
</span><span class="n">confusion_matrix</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'fileName.csv'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[TL;DR: In applied machine learning, models are often optimised for their accuracy in predicting a target. When the target is quantitative, measures like the \(R^2\) value and the Root Mean Squared Error (RMSE) etc., help in quantifying this accuracy. When the target is qualitative, metrics like Accuracy, Precision, Recall and several others are used. We term these measures classification metrics for the sake of this blog post.]]></summary></entry><entry><title type="html">Deep Learning deployment on AWS</title><link href="https://nxrmrz.github.io/blog/2020/aws-bert-deployment/" rel="alternate" type="text/html" title="Deep Learning deployment on AWS"/><published>2020-09-28T09:00:16+00:00</published><updated>2020-09-28T09:00:16+00:00</updated><id>https://nxrmrz.github.io/blog/2020/aws-bert-deployment</id><content type="html" xml:base="https://nxrmrz.github.io/blog/2020/aws-bert-deployment/"><![CDATA[<p>My first technical Medium article has been published!ðŸŽ‰ðŸŽ‰</p> <p><a href="https://medium.com/chnnl/deploying-an-externally-trained-deep-learning-model-for-batch-inference-in-aws-2d0b21a0b7cd">Linked here</a>, Itâ€™s about deploying a transformer natural language processing model Iâ€™ve built for a start-up Iâ€™ve worked for in the past called <a href="https://www.chnnl.app/">chnnl</a>. This open source model (BERT) is written in PyTorch and deployed onto an AWS Managed Container for batch inference. The model reads text and outputs an emotion and sentiment label. I thought Iâ€™d write about it as documentation on this topic is very sparse at this time from AWSâ€™ side.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[My first technical Medium article has been published!ðŸŽ‰ðŸŽ‰]]></summary></entry><entry><title type="html">Detecting seasons with React.js</title><link href="https://nxrmrz.github.io/blog/2020/react-first-project/" rel="alternate" type="text/html" title="Detecting seasons with React.js"/><published>2020-04-05T17:00:00+00:00</published><updated>2020-04-05T17:00:00+00:00</updated><id>https://nxrmrz.github.io/blog/2020/react-first-project</id><content type="html" xml:base="https://nxrmrz.github.io/blog/2020/react-first-project/"><![CDATA[<p>Hereâ€™s a proud moment - the first front-end Iâ€™ve built with React.js and JSX. Itâ€™s very simple, but it taught me a lot on how React worked internally.</p> <p>This app displays either two screens dependent on whether itâ€™s summer or winter wherever the user is currently.</p> <p>A user in New Zealand (i.e. me) is in the southern hemisphere where itâ€™s currenty winter (May 2nd, when this blogpost was written). The app displays the winter screen in this case.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-04-05-react-first-project/winterscreen-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-04-05-react-first-project/winterscreen-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-04-05-react-first-project/winterscreen-1400.webp"/> <img src="/assets/img/2020-04-05-react-first-project/winterscreen.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A user in San Francisco, on the other hand, is in the northern hemisphere where itâ€™s currently summer. The app displays the summer screen for them.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-04-05-react-first-project/summerscreen-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-04-05-react-first-project/summerscreen-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-04-05-react-first-project/summerscreen-1400.webp"/> <img src="/assets/img/2020-04-05-react-first-project/summerscreen.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>How do we know where the user is? Well, we can detect user location with the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Geolocation_API">Mozilla Geolocation API</a>. We can grab latitude and longitude information about the user from this service.</p> <p>We can also force the Google Chrome browser to consider a location like San Francisco away from us by changing its geolocation in its â€˜Sensorsâ€™ tab like so (mostly for testing the different views):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-04-05-react-first-project/forcelocationchange-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-04-05-react-first-project/forcelocationchange-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-04-05-react-first-project/forcelocationchange-1400.webp"/> <img src="/assets/img/2020-04-05-react-first-project/forcelocationchange.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If the user has disabled location-sharing in any-way, the screen below encourages them to enable it</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-04-05-react-first-project/acceptlocationscreen-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-04-05-react-first-project/acceptlocationscreen-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-04-05-react-first-project/acceptlocationscreen-1400.webp"/> <img src="/assets/img/2020-04-05-react-first-project/acceptlocationscreen.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Semantic UI and some CSS was used to style the webpage.</p> <p>This project taught me Reactâ€™s fundamentals - class based and functional components, the component life cycle methods, how to update component state, and how to pass information down from parent to children components using the props system.</p> <p>Iâ€™m currently building and deploying a more complex full-stack web app, integrating node.js/express.js for the back-end and a mongodb database. I might write a tutorial on that when itâ€™s done, so if youâ€™re interested, please stay tuned for that post :)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Hereâ€™s a proud moment - the first front-end Iâ€™ve built with React.js and JSX. Itâ€™s very simple, but it taught me a lot on how React worked internally.]]></summary></entry><entry><title type="html">Scraping Glassdoor Salaries</title><link href="https://nxrmrz.github.io/blog/2020/data-scraping/" rel="alternate" type="text/html" title="Scraping Glassdoor Salaries"/><published>2020-03-28T11:40:16+00:00</published><updated>2020-03-28T11:40:16+00:00</updated><id>https://nxrmrz.github.io/blog/2020/data-scraping</id><content type="html" xml:base="https://nxrmrz.github.io/blog/2020/data-scraping/"><![CDATA[<p>An ex-colleague, <a href="https://www.linkedin.com/in/dhilip-subramanian-36021918b/">Dhilip</a> and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one Iâ€™ve thought about for quite some time.</p> <p>It was a timely project for us Qrious interns, who were deliberating on our next career move following the summer internship. Towards the end of our internship, a few of us (excluding me but including Dhilip) were graduating and applying for full-time jobs in the data space. Our discussions naturally gravitated to what starting pay we could expect for careers in data science, data engineering, and analytics. I imagined if we had an amount for each job averaged from recent, accurate salary data collected over a large number of companies operating in different industries, we had evidence to make sure future salary negotiations were fair!</p> <h3 id="the-execution">The Execution</h3> <p>Glassdoor is a job reviews site with job benefits and salary information reported anonymously by employees of various companies. It was our main data source for this project. Collecting a vast amount of data from the site would require automation, using a technique called <em>scraping</em>. This is a technique used to extract content from specific HTML tags in a webpage, and in our case, exploits two technologies to do so: Selenium and BeautifulSoup.</p> <p>Selenium is a Java-based tool used in website testing to automate specific interactions with webpages (i.e. clicking on links, logging in, navigating the page, etc). Since it automates interacting with the DOM, itâ€™s being used in Data Science to scrape data from websites. BeautifulSoup, on the other hand, is a Python library that parses HTML and makes it easy to extract specific elements from it.</p> <h3 id="the-code">The code</h3> <h4 id="1-first-we-import-the-necessary-libraries">1. First, we import the necessary libraries:</h4> <p><em>Python Selenium libraries (Scraping)</em></p> <ul> <li>webdriver - contains tools for working with an automated web browser</li> <li>webdriver.chrome - specifies Google Chrome as our automated browser</li> <li>time - the sleep function allows our automated tasks (i.e. clicking on links) to have a delay. Important for not being blocked as a bot by some websites</li> <li>BeautifulSoup - a python library for parsing HTML pages and grabbing content from specific HTML tags easily</li> <li>lxml - a parser for BeautifulSoup, allowing the HTML pages to be parsed as xml (and queried with xPath)</li> </ul> <p><em>Standard Python Libraries</em></p> <ul> <li>pandas - a library for working with data-frames</li> <li>csv - a tool for reading and writing CSV files</li> <li>itertools - an iterator library</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">selenium</span> <span class="kn">import</span> <span class="n">webdriver</span> 
<span class="kn">from</span> <span class="nn">selenium.webdriver.chrome.options</span> <span class="kn">import</span> <span class="n">Options</span>
<span class="kn">from</span> <span class="nn">webdriver_manager.chrome</span> <span class="kn">import</span> <span class="n">ChromeDriverManager</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">lxml</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> 
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">zip_longest</span>
</code></pre></div></div> <h4 id="2-then-we-set-up-an-automated-browser-for-scraping">2. Then, we set up an automated browser for scraping</h4> <p>In this step, we navigate to <a href="https://www.glassdoor.co.nz/Salaries/index.htm">Glassdoorâ€™s Salaries page</a> and type in whatever Job Title weâ€™d want salaries for, in whatever location. In this example, weâ€™d want to grab Data Engineersâ€™ salary information from the United States. We have to be logged into a Glassdoor account to do this.</p> <p>We then copy paste the URL of the resulting page into the driver.get(url) method below.</p> <p>Next, we find the <em>last page of results</em> Glassdoor has for this particular search. This is important as weâ€™re setting up our browser to automatically cycle through all pages of the search, grabbing salary information from each. We do this by modifying the URL in our browser, adding the string <em>_IP1500</em> just before the <em>.htm</em>. This lets us jump to the very last page of salary information, as it is likely that there wonâ€™t be 1500 pages worth of search results. If there is, just adjust the IP number to be larger.</p> <p>Once weâ€™ve jumped to the last page of results, note down what that page is (by looking at the last number in the carousel button as such (red in the image below):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-03-28/LastPageScrape-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-03-28/LastPageScrape-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-03-28/LastPageScrape-1400.webp"/> <img src="/assets/img/2020-03-28/LastPageScrape.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>and copying that number onto the lastPageNo variable in the below code. In belowâ€™s example, page 191 is the last page, and the for loop cycles through each page until it reaches that, employing a delay of at least 1.5s before it goes on to the next page.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#creating empty arrays to hold job title, company name, job mean pay and pay range information
</span><span class="n">job_title</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">company_name</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mean_pay</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pay_range</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">lastPageNo</span> <span class="o">=</span> <span class="mi">191</span><span class="p">;</span>

<span class="c1">#going through 184 pages of salary information
</span><span class="k">for</span> <span class="n">pageno</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">lastPageNo</span><span class="p">):</span>

    <span class="n">driver</span> <span class="o">=</span> <span class="n">webdriver</span><span class="p">.</span><span class="n">Chrome</span><span class="p">(</span><span class="n">ChromeDriverManager</span><span class="p">().</span><span class="n">install</span><span class="p">())</span>
    
    <span class="c1">#getting webpage in glassdoor
</span>    <span class="k">if</span> <span class="n">pageno</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">driver</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">driver</span><span class="p">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s">"https://www.glassdoor.co.nz/Salaries/us-data-engineer-salary-SRCH_IL.0,2_IN1_KO3,16.htm"</span> <span class="o">+</span> <span class="s">"_IP"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">pageno</span><span class="p">)</span> <span class="o">+</span> <span class="s">".htm"</span>
        <span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
</code></pre></div></div> <h4 id="3-we-parse-the-html-of-each-search-result-pageand-scrape">3. We parse the HTML of each search result pageâ€¦and SCRAPE!</h4> <p>â€¦Using Beautifulsoup. The <em>page_source</em> attribute of the driver grabs the page with its corresponding HTML tags, and parses it with <strong>lxml</strong>, which as mentioned above, allows us to query the results using xpath if we wished.</p> <p>After parsing, we then grab specific HTML content. We do this by:</p> <ul> <li>Inspecting the html tags where our information lies, by using Google Chromeâ€™s <em>inspect</em> option</li> <li>Collecting information that would distinguish our target HTML tag/s from others</li> </ul> <p>Tag Classes and IDs are useful for this. We see in the below screenshot, for example, that each salary block is enclosed by a <code class="language-plaintext highlighter-rouge">&lt;div&gt;</code> with class <strong>â€œrow align-items-center m-0 salaryRow__SalaryRowStyle__rowâ€</strong>. To grab each salary block from a page, we then use BeautifulSoupâ€™s findAll() method, passing on the <code class="language-plaintext highlighter-rouge">&lt;div class=""&gt;</code> information mentioned.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2020-03-28/salaryBlocks-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2020-03-28/salaryBlocks-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2020-03-28/salaryBlocks-1400.webp"/> <img src="/assets/img/2020-03-28/salaryBlocks.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We do the same for every piece of information we want (i.e. job titles, company name, average salary, and salary range in this example). These bits of info were obtained the same way as above. We looped through each salary block above to grab the specific information, as the below code shows.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">#continuation from code above (still inside the for loop)
</span>    <span class="c1">#parsing the page through lxml option of beautifulsoup
</span>    <span class="n">html</span> <span class="o">=</span> <span class="n">driver</span><span class="p">.</span><span class="n">page_source</span>
    <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="s">'lxml'</span><span class="p">)</span>

    <span class="c1">#getting each salary block
</span>    <span class="n">salaryBlocks</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'row align-items-center m-0 salaryRow__SalaryRowStyle__row'</span><span class="p">})</span>

    <span class="c1">#for each salary block, find the job title, company name, average pay, and pay range, and append them to the lists initialised above
</span>    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">salaryBlocks</span><span class="p">:</span>
        <span class="n">entry</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">jobTitle</span> <span class="o">=</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'salaryRow__JobInfoStyle__jobTitle strong'</span><span class="p">}).</span><span class="n">find</span><span class="p">(</span><span class="s">"a"</span><span class="p">).</span><span class="n">text</span>
        <span class="n">job_title</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">jobTitle</span><span class="p">)</span>

        <span class="n">companyName</span> <span class="o">=</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'salaryRow__JobInfoStyle__employerName'</span><span class="p">}).</span><span class="n">text</span>
        <span class="n">company_name</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">companyName</span><span class="p">)</span>

        <span class="n">meanPay</span> <span class="o">=</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'salaryRow__JobInfoStyle__meanBasePay common__formFactorHelpers__showHH'</span><span class="p">}).</span><span class="n">find</span><span class="p">(</span><span class="s">'span'</span><span class="p">).</span><span class="n">text</span>
        <span class="n">mean_pay</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">meanPay</span><span class="p">)</span>
        
        <span class="c1">#if a pay range exists, grab it, otherwise, indicate none exists
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'</span><span class="p">}).</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'strong'</span><span class="p">}):</span>
                <span class="n">payRange</span> <span class="o">=</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'</span><span class="p">}).</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'strong'</span><span class="p">}).</span><span class="n">text</span>
                <span class="n">pay_range</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">payRange</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">block</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"div"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'col-2 d-none d-md-block px-0 py salaryRow__SalaryRowStyle__amt'</span><span class="p">}).</span><span class="n">find</span><span class="p">(</span><span class="s">"span"</span><span class="p">,</span> <span class="p">{</span><span class="s">'class'</span> <span class="p">:</span> <span class="s">'strong'</span><span class="p">}):</span>
                <span class="n">pay_range</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"N/A"</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">pay_range</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"N/A"</span><span class="p">)</span>

        <span class="n">driver</span><span class="p">.</span><span class="n">quit</span><span class="p">()</span>
</code></pre></div></div> <h4 id="4-we-save-the-results-to-a-csv-file">4. We save the results to a .csv file</h4> <p>Once weâ€™ve obtained all the information we need, we store them into a python data-frame, which allows us to store the data in a tabular format. The columns of the table correspond to job title, company name, mean pay, and the pay range, whilst the rows are individual companies.</p> <p>The below code shows how the results are stored in a data-frame and eventually converted to a .csv file for easy reading into your favourite data analysis program/language later on.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#process the lists into a final dataframe, and save to a CSV
</span><span class="n">final</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">zip_longest</span><span class="p">(</span><span class="n">job_title</span><span class="p">,</span> <span class="n">company_name</span><span class="p">,</span> <span class="n">mean_pay</span><span class="p">,</span> <span class="n">pay_range</span><span class="p">):</span>
    <span class="n">final</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">final</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'jobTitle'</span><span class="p">,</span> <span class="s">'companyName'</span><span class="p">,</span> <span class="s">'meanPay'</span><span class="p">,</span> <span class="s">'payRange'</span><span class="p">])</span>

<span class="n">df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"Data Engineer Salaries United States.csv"</span><span class="p">)</span>
</code></pre></div></div> <p><em>The final output of this scraping is a 28,000 row file, containing salary information for Data Engineers, Analysts, Scientists, and Machine Learning Engineers in Australia, New Zealand, and the United States. The file can be downloaded here for free :)</em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[An ex-colleague, Dhilip and I wanted to do a series of end-to-end projects in Data Science and Analytics for fun and to pick up skills along the way that would serve our future careers in this space. When Dhilip approached me for project ideas, I already had one Iâ€™ve thought about for quite some time.]]></summary></entry></feed>